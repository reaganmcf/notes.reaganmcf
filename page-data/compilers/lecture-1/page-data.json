{
    "componentChunkName": "component---src-pages-markdown-remark-frontmatter-slug-tsx",
    "path": "/compilers/lecture-1/",
    "result": {"data":{"markdownRemark":{"html":"<h1>Compilers</h1>\n<ul>\n<li>What is a <strong>compiler</strong>?\n<ul>\n<li>A program that translates an <em>executable</em> program in one language into an <em>executable</em> program in another language</li>\n<li>A good compiler should improve the program, <em>in some way</em></li>\n</ul>\n</li>\n<li>What is an <strong>interpreter</strong>?\n<ul>\n<li>A program that reads an <em>executable</em> program and produces the results of executing that program</li>\n</ul>\n</li>\n<li>C is typically compiled, Scheme is typically interpreted</li>\n<li>Java is compiled to bytecode (for the Java VM)\n<ul>\n<li>Which is then interpreted</li>\n<li>Or a hybrid strategy is used\n<ul>\n<li>Just-in-time compilation</li>\n<li>Dynamic optimization (hot paths)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Why Study Compilation?</h2>\n<ul>\n<li>Compilers are important system software components\n<ul>\n<li>They are intimately interconnected with architecture, systems, programming methodology, and language design</li>\n</ul>\n</li>\n<li>Compilers include many applications of theory to practice\n<ul>\n<li>Scanning, parsing, static analysis, instruction selection</li>\n</ul>\n</li>\n<li>Many practical applications have embedded languages\n<ul>\n<li>Commands, macros</li>\n</ul>\n</li>\n<li>Many applications have input formats that look like languages\n<ul>\n<li>Matlab, Mathematica</li>\n</ul>\n</li>\n<li>Writing a compiler exposes practical algorithmic &#x26; engineering issues\n<ul>\n<li>Approximating hard problems; efficiency and scalability</li>\n<li>No free lunch, i.e., there are multi-dimensional trade-offs</li>\n</ul>\n</li>\n</ul>\n<h2>Intrinsic interest</h2>\n<ul>\n<li>Compiler construction involves ideas from many different parts of computer science</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Discipline</th>\n<th>Topics</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Artificial Intelligence</td>\n<td>Greedy algorithms, Heuristic search, ML</td>\n</tr>\n<tr>\n<td>Algorithms</td>\n<td>Graph algorithms, union-find, DP, approximations</td>\n</tr>\n<tr>\n<td>Theory</td>\n<td>DFAs &#x26; PDAs, pattern matching, fixed-point algos</td>\n</tr>\n<tr>\n<td>Systems</td>\n<td>Allocation &#x26; naming, synchronization, data locality</td>\n</tr>\n<tr>\n<td>Architecture</td>\n<td>Pipeline &#x26; hierarchy management, instruction set use, parallelism, quantum computing</td>\n</tr>\n</tbody>\n</table>\n<h2>Intrinsic merit</h2>\n<ul>\n<li>Compiler: construction poses challenging and interesting problems\n<ul>\n<li>Compilers must do a lot but also <strong>run fast</strong></li>\n<li>Compilers have primary responsibility for <strong>run-time performance</strong></li>\n<li>Compilers are responsible for making it acceptable to use the <strong>full power</strong> of the programming language</li>\n<li>Computer architects perpetually create new challenges for the compiler by building more <strong>complex machines</strong> (e.g.: multi-core, GPUs, FPGAs, quantum computers, neuromorphic processors)</li>\n<li>Compilers must/should hide that complexity from the programmer</li>\n<li>Success requires mastery of complex interactions</li>\n</ul>\n</li>\n</ul>\n<h2>Making languages usable</h2>\n<blockquote>\n<p>It was our belief that if FORTRAN, during its first months, were to translate any reasonable “scientific” source program into an object program only half as fast as its hand-coded counterpart, then acceptance of our system would be in serious danger.. I believe that had we failed to produce efficient programs, the widespread use of languages like FORTRAN would have been seriously delayed.</p>\n</blockquote>\n<p>- John Backus</p>\n<h2>High-level View of a Compiler</h2>\n<p><img src=\"https://i.gyazo.com/e7685fddf81282b6e74eb0faa9e147af.png\" alt=\"\"></p>\n<p>Implications:</p>\n<ul>\n<li>Must recognize legal (and illegal) programs</li>\n<li>Must generate correct code</li>\n<li>Must manage storage of all variables (and code)</li>\n<li>Must agree with OS &#x26; linker on format for object code</li>\n<li>Big step up from assembly language - use higher level notations</li>\n</ul>\n<h2>Traditional Two-pass Compiler</h2>\n<p><img src=\"https://i.gyazo.com/b1e47ab4bbb2aa30565311f637a97f68.png\" alt=\"\"></p>\n<p>Implications:</p>\n<ul>\n<li>Use an intermediate representation (IR)</li>\n<li>Front end maps legal source code into IR</li>\n<li>Back end maps IR into target machine code</li>\n<li>Extension: multiple front ends &#x26; multiple passes (better code)</li>\n<li>Typically, front end is O(N) or O(n log n), while back end is NP-complete</li>\n</ul>\n<h3>A Common Fallacy</h3>\n<p><img src=\"https://i.gyazo.com/afe010faa4972eef865754a8c87a4695.png\" alt=\"\"></p>\n<p>Can we build n x m compilers with n + m components?</p>\n<ul>\n<li>Must encode all language specific knowledge in each front end</li>\n<li>Must encode all features in a single IR</li>\n<li>Must encode all target specific knowledge in each back end</li>\n<li>Limited success in systems with very low-level IRs</li>\n</ul>\n<h2>The Front End</h2>\n<p><img src=\"https://i.gyazo.com/681184ea09d807ae0da65f1792e7563e.png\" alt=\"\"></p>\n<p>Responsibilities:</p>\n<ul>\n<li>Recognize legal (&#x26; illegal) programs</li>\n<li>Report errors in a useful way</li>\n<li>Produce IR &#x26; preliminary storage map</li>\n<li>Shape the code for the back end</li>\n<li>Much of front end construction can be automated</li>\n</ul>\n<h3>Scanner</h3>\n<ul>\n<li>Maps character stream into words - the basic unit of syntax</li>\n<li>Produces pairs - a word &#x26; its part of speech\n<ul>\n<li><code class=\"language-text\">x = x + y;</code> becomes <code class=\"language-text\">&lt;id,x> = &lt;id,x> + &lt;id,y>;</code></li>\n<li>word ~= lexeme, part of speech ~= token type</li>\n<li>In casual speech, we call the pair a <em>token</em></li>\n</ul>\n</li>\n<li>Typical tokens include <em>number</em>, <em>identifier</em>, <em>+</em>, <em>-</em>, <em>new</em>, <em>while</em>, <em>if</em></li>\n<li>Scanner eliminates white space (including comments)</li>\n<li>Speed is important</li>\n</ul>\n<h3>Parser</h3>\n<ul>\n<li>Recognizes context-free syntax &#x26; reports errors</li>\n<li>Guides context-sensitive (“semantic”) analysis (type-checking)</li>\n<li>Builds IR for source program</li>\n<li>Hand coded parsers are fairly easy to build</li>\n<li>Most books advocate using automatic parser generators</li>\n</ul>\n<h3>Context-free Syntax</h3>\n<p>Context-free syntax is specified with a grammar</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">SheepNoise -> SheepNoise baa\n                  | baa</code></pre></div>\n<p>This grammar defines the set of noises that a sheep makes under normal circumstances.</p>\n<p>It is written in a variant of Backus-Naur Form (BNF)</p>\n<p>Formally, a grammar $G = (S, N, T, P)$</p>\n<ul>\n<li>$S$ is the <em>start symbol</em></li>\n<li>$N$ is the set of <em>non-terminal symbols</em></li>\n<li>$T$ is a set of <em>terminal symbols</em> or <em>words</em></li>\n<li>$P$ is a set of <em>produictions</em> or <em>rewrite rules</em> ($P: N \\rarr N \\cup T$)</li>\n</ul>\n<p>Context-free syntax can be put to better use</p>\n<h3>Context-free Grammar</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1. goal -> expr\n2. expr -> expr op term\n3.       | term\n4. term -> number\n5.       | id\n6. op   -> +\n7.       | -</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">S = goal\nT = { number, id, +, - }\nN = { goal, expr, term, op }\nP = { 1, 2, 3, 4, 5, 6, 7 }</code></pre></div>\n<ul>\n<li>This grammar defines simple expressions with addition &#x26; subtraction over “number” and “id”</li>\n<li>This grammar, like many, falls in a class called “context-free grammars”, abbreviated CFG</li>\n</ul>\n<p>Given a CFG, we can <strong>derive</strong> sentences by repeated substitution</p>\n<p><img src=\"https://i.gyazo.com/9cf7f929afe3b2ccc6226601b8c2c0df.png\" alt=\"\"></p>\n<p>To recognize a valid sentence in some CFG, we will need to construct a derivation automatically (forward or backwards)</p>\n<h3>Parse Trees</h3>\n<p>A parse can be represented by a tree (<strong>parse tree</strong> or <strong>syntax tree</strong>)</p>\n<p><img src=\"https://i.gyazo.com/4044d554e82dbdf9e7db152009a3153e.png\" alt=\"\"></p>\n<p>Compilers often use an <strong>abstract syntax tree (AST)</strong></p>\n<p><img src=\"https://i.gyazo.com/11306b8470b8286c4c7a663237dd9524.png\" alt=\"\"></p>\n<ul>\n<li>The AST summarizes grammatical structure, without including detail about the derivation</li>\n<li>This form is much more concise</li>\n<li>ASTs are one kind of <em>intermediate representation (IR)</em></li>\n</ul>\n<h2>The Back End</h2>\n<p><img src=\"https://i.gyazo.com/38beca6ba450f71de49aff690f4342be.png\" alt=\"\"></p>\n<p>Responsibilities:</p>\n<ul>\n<li>Translate IR into target machine code</li>\n<li>Choose instructions to implement each IR operation</li>\n<li>Decide which value to keep in registers</li>\n<li>Ensure conformance with system interfaces</li>\n<li>Automation has been <em>less</em> successful in the back end</li>\n</ul>\n<h3>Instruction selection</h3>\n<ul>\n<li>Produce fast, compact code</li>\n<li>Take advantage of target features such as addressing models</li>\n<li>Usually viewed as a pattern matching problem\n<ul>\n<li><em>ad hoc</em> methods, pattern matching, dynamic programming</li>\n</ul>\n</li>\n<li>This was the problem of the future in 1978\n<ul>\n<li>Spurred by transition from PDP-11 to VAX-11</li>\n<li>Orthogonality of RISC simplified this problem</li>\n</ul>\n</li>\n</ul>\n<h3>Register Allocation</h3>\n<ul>\n<li>Have each value in a register when it is used</li>\n<li>Manage a limited set of resources</li>\n<li>Select appropriate <code class=\"language-text\">LOAD</code>s and <code class=\"language-text\">STORE</code>s</li>\n<li>Optimal allocation is NP-complete</li>\n</ul>\n<p>Typically, compilers approximate solutions to NP-Complete problems</p>\n<h3>Instruction Scheduling</h3>\n<ul>\n<li>Avoid hardware stalls and interlocks</li>\n<li>Use all functional units productively</li>\n<li>Can increase lifetime of variables (changing the allocation)</li>\n</ul>\n<p>Optimal scheduling is NP-Complete in nearly all cases\nHeuristic techniques are well developed</p>\n<h2>Traditional Three-pass Compiler</h2>\n<p><img src=\"https://i.gyazo.com/b3b9d24c41b427fcd7ef40f388a4681f.png\" alt=\"\"></p>\n<p>Code improvement (or <strong>Optimization</strong>)</p>\n<ul>\n<li>Analyzes IR and rewrites (or transforms) IR</li>\n<li>Primary goal is to reduce running time of the compiled code\n<ul>\n<li>May also improve space, power dissipation, energy consumption</li>\n</ul>\n</li>\n<li>Must preserve “meaning” of the code (may include approximations, i.e., quality of outcomes trade-offs)</li>\n</ul>\n<h3>The Optimizer (or Middle End)</h3>\n<p><img src=\"https://i.gyazo.com/04263bdbd07d7f6c847f2c2907e08e8d.png\" alt=\"\"></p>\n<p>Typical Transformations</p>\n<ul>\n<li>Discover &#x26; propagate some constant value</li>\n<li>Move a computation to a less frequently executed place</li>\n<li>Specialize some computation based on context</li>\n<li>Discover a redundant computation &#x26; remove it</li>\n<li>Remove useless or unreachable code</li>\n<li>Encode an idiom in some particularly efficient form</li>\n</ul>\n<h2>Modern Restructuring Compiler</h2>\n<p><img src=\"https://i.gyazo.com/c8e152f11c795d771667171d7a376fa1.png\" alt=\"\"></p>\n<p>Typical <strong>Restructuring</strong> (source-to-source) Transformations:</p>\n<ul>\n<li>Blocking for memory hierarchy and register reuse</li>\n<li>Vectorization</li>\n<li>Parallelization</li>\n<li>All based on dependence</li>\n<li>Also full and partial inlining</li>\n</ul>\n<h2>Rule of the Run-time system</h2>\n<ul>\n<li>Memory management services\n<ul>\n<li>Allocate\n<ul>\n<li>In the heap or in an activation record (stack frame)</li>\n</ul>\n</li>\n<li>Deallocate</li>\n<li>Collect garbage</li>\n</ul>\n</li>\n<li>Run-time type type-checking</li>\n<li>Error processing (exception handling)</li>\n<li>Interface to the operating system\n<ul>\n<li>Input and output</li>\n</ul>\n</li>\n<li>Support for parallelism\n<ul>\n<li>Parallel thread initiation</li>\n<li>Communication and synchronization</li>\n</ul>\n</li>\n</ul>","frontmatter":{"date":"January 19, 2021","slug":"/compilers/lecture-1","title":"Compilers - Lecture 1"}}},"pageContext":{"id":"c5352cb6-664e-5923-836c-0a7ea2f83d0b","frontmatter__slug":"/compilers/lecture-1","__params":{"frontmatter__slug":"compilers"}}},
    "staticQueryHashes": ["1727495174"]}