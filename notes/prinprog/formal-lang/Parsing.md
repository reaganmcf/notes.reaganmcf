# Parse Trees
![](https://i.gyazo.com/3c8321832679513949f5b2ad3eef5004.png)
- A parse tree shows the _derivation_ of a string
- Parsing attempts to construct a parse tree for any string in the language
- A grammar is _deterministic_ if every string in the language is generated by exactly one parse tree
- With a deterministic grammar and a parser, we can use the parse tree to interpret the string

## Example
![](https://i.gyazo.com/2955cc1499e7805e45cacb22642b486f.png)
- This parse tree shows that "1 + 2 * x" is part of $L_E$
- We can use the parse tree to guide our interpretation (e.g., multiply before add)
	- But it contains a lot of needless detail
- Instead of constructing an actual parse tree, parsers will associate a function with associate a function with each rule
	- Essentially, we fold over the tree as we produce it

## Problems with Parse Trees
- There are lots of irrelevant details in the parse tree
- What do we create instead of a parse tree?
- For simple languages describing computation, we could compute the result
- Generally, we will produce an _abstract syntax tree_ (AST)
- The AST captures the essential structure of the input without getting bogged down in irrelevant detail
	- E.g., we don't need to represent (1 + 2) + 3 differently from 1 + 2 +3

## Abstract Syntax Trees
![](https://i.gyazo.com/821e83fc29938cfd31ceeb9a146c39e3.png)
- The AST represents the meaning of a string in a more compact way convenient for a program(mer) to work with
- Its design and use is beyond the scope of parsing and formal languages
 
 ## Recursive-Descent Parsers
 - There are many approaches to writing ap arser
 - Most hand-written parsers are _recursive-descent_ parsers 
	 - For each non-terminal, write a function that parses part of the string and returns whatever value
	 - In Haskell, we can make a monad or applicative functor `Parser a` and build parsers out of smaller parsers
		 - `pExpr = (+) <$> pFactor <* token "+" <*> pExpr <|> pFactor`
		 - A parser could be a function that takes an input string and returns a list of possible values, along with the unparsed part of the string (which is passed to the next parser)
		 - `newtype Parser a = Parser (String -> [(a, String)])`
 - Recursive-descent parsers are convenient, but have some drawbacks
	 - We must rewrite CFGs to eliminate _left recursion_, to avoid infinite loops
	 - Hard to get high-level analysis of parser (e.g., detecting ambiguity)
	 - Choose between overhead of supporting backtracking/multiple results
		 - ...or limit ourselves to grammars where each choice is determined by the first symbol
	 - On the other hand, we can write recursive-descent parsers for context-_dependent_ langauges

## Parser Generators
- Specify grammar in some other language (BNF), use tool to create parser
	- Often can include "semantic actions" to generate AST
- Yacc (Yet Another Compiler Compiler), Bison, Menhir, many others
- Able to analyze grammar, detect ambiguity
- Downsides:
	- Another language you have to learn
	- Another tool you need to compile your program

## Classifying Grammar
- Not every parsing system can handle every CFG
- Some systems (Earley parsers) can make a parser for any CFG, and return all parse trees for ambiguous grammars
	- Earley parsers are $O(n^3)$ in the size of the string
- LR(1) parsers can handle any unambiguous CFG, if rewritten to avoid _right recursion_
	- Invented in 1965 by Donald Knuth
	- LR parsers are $O(n)$ in the size of the string
	- LR(k) = "left to right parsing, rightmost derivation, k token look ahead"

## Lexical Syntax
- LR parsers usually work with _tokens_ rather than symbols
	- Each keyword and operator might be a token, with other tokens standing for identifier or literal
	- This greatly reduces the number of rules, and (thus) the size of the parser
- A _lexer_ is a parser used to transform a sequence of characters into a sequence of tokens
	- The lexer is usually specified using [[Regular Expressions]]
- Separating the lexer from the parser frees us to define tokens that are difficult/inefficient to specify using a CFG

## Big Picture: Compiler Front-End
- There are three major reasons a compiler might reject our program
- A _lexical error_ occurs if our program cannot be converted into tokens (e.g., an unclosed string)
- A _syntax error_ occurs if our token stream is not produced by the grammar
- Static analysis finds _semantic errors_
	- Use of undeclared names, incorrect types, incorrect number of arguments to functions, etc.
- The back-end transforms the IR to the target language

### Example:
```
Expr5 ::= Identifier | Literal | "(" Expr ")"
Expr4 ::= Expr4 "*" Expr5 | Expr5
Expr3 ::= Expr3 "+" Expr4 | Expr4
Expr2 ::= Expr2 "==" Expr3 | "!" Expr2 | Expr3
Expr1 ::= Expr1 "&&" Expr2 | Expr2
Expr  ::= Expr "||" Expr1 | Expr1

Stmt ::= Identifier ":=" Expr ";"
Stmt ::= "while" "(" Expr ")" Stmt;
Stmt ::= "if" "(" Expr ")" Stmt "else" Stmt
Stmt ::= "if" "(" Expr ")" Stmt
Stmt ::= "{" Stmts "}"

Stmts ::= Stmt Stmts | Stmt
```

- Defines both lexical syntax and grammar
- Each quoted term is a keyword token
- _Identifier_ and _Literal_ are presumably defined elsewhere
- A parser generator will helpfully tell us this grammar is ambiguous
	- ...although it usually won't say _why_
	- It's because "if (A) if (B) X else Y" can be parsed two ways
	
## Case Study
- Many available parser frameworks for Haskell
	- Some are recursive descent (e.g., Parsec)
	- Some are parser generators (e.g., Happy) and lexers (e.g., Alex)
	- Some are embedded generators (e.g., Earley)
- We will use BNFC-meta, which uses _Labeled_ BNF, a variant that gives a name to each rule
	- Given an LBNF grammar, BNFC generates a parser, a AST, and a function to turn the AST back into the source language (pretty printer)

#### Aside: Metaprogramming
- A meta-program is a program that writes programs
	- The C pre-processor can be used to generate fragments of C code
- GHC offers _Template Haskell_
	- During compilation, run code that generates Haskell code and splice the result into your program
	- Works with AST, not source code
- GHC also offers "quasi-quoters"
	- Parse arbitrary strings during compilation and splice resulting Haskell code into program

## BNFC-meta Example
```hs
bnfc [lbnf|
	EVar.	Expr5 ::= Ident;
	Elit.	Expr5 ::= Literal;
	ETimes.	Expr4 ::= Expr4 "*" Expr5;
	EPlus.	Expr3 ::= Expr3 "+" Expr4;
	EEQ.	Expr2 ::= Expr2 "==" Expr3;
	ENot.	Expr2 ::= "!" Expr2;
	EAnd.	Expr1 ::= Expr1 "&&" Expr2;
	EOr.	Expr  ::= Expr  "||" Expr1;
	_. Expr5 ::= "(" Expr ")";
	_. Expr4 ::= Expr5;
	_. Expr3 ::= Expr4;
	_. Expr2 ::= Expr3;
	_. Expr1 ::= Expr2;
	_. Expr  ::= Expr1;
	
	SAssign. Stmt ::= Ident ":=" Expr ";";
	SWhile.	 Stmt ::= "while" "(" Expr ")" Stmt;
	SIfElse. Stmt ::= "if" "(" Expr ")" Stmt;
	SBlock.	 Stmt ::= "{" [Stmt] "}";
	
	(:[]). Stmts ::= Stmt;
	(:).   Stmts ::= Stmt Stmts;
|]
```
- `[lbnf| ... |]` is a quasi-quote
- The function `lbnf` runs at compile time and turns the string into Haskell code creating a `Grammar`
- `bnfc` takes a `Grammar` and returns Haskell code that defines parsers, AST types, and pretty printers
	- Its results are spliced into the program at compile time
- LBNF rules have the form:
	- `Name. Symbol ::= Symbols;`
- BNFC uses Name when creating the AST data type
	- Symbols like `Expr1`, `Expr2`, etc., generate the same type (`Expr`)
- `_`, `(:[])`,  and `(:)` are treated specially
- The code above defines
	- Types `Stmt` and `Expr`
	- Parsers `pStmt` and `pExpr`
		- `pStmt . tokens :: String -> ParseMonad Stmt`
		- `ParseMonad a` may contain an `a` (on success) or a string (on failure)
	- Quasi-quoters `stmt` and `expr`
		- In another file, we can write `[stmt| x := x + 1; |]` and it will be parsed at compile time and replaced with a `Stmt`